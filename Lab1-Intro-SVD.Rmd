---
title: "Analysis of High Dimensional Data - Lab 1"
author: "Adapted by Milan Malfait"
date: "15 Oct 2020"
output:
    html_document:
      code_download: true
      theme: cosmo
      toc: true
      toc_float: true
      highlight: tango
      number_sections: true
---

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

***

# Introduction

The purpose of the following exercises is mainly to get more familiar with SVD and its applications, in particular Multidimensional Scaling (MDS).
It is recommended to perform the exercises in an `RMarkdown` document.

For a brief introduction to RMarkdown, see [Introduction to RMarkdown](./Introduction-RMarkdown.html).

For an introduction to working with matrices in R, see [Working with Matrices in R](./Introduction-Matrices-R.html).


## Libraries {-}

Packages used in this document.
Installation code is commented, uncomment and paste this code in an R console to install the packages.

```{r libraries, message=FALSE, warning=FALSE}
# install.packages(c("tidyverse", "ggfortify"))
library(tidyverse)
library(ggfortify)
```



# Multidimensional Scaling (MDS) demonstration

See [course notes](https://statomics.github.io/HDA2020/pages/svd.html#7_SVD_and_Multi-Dimensional_Scaling_(MDS)) for background.


* We will use `UScitiesD` data as an example
* Our goal is to use the distance matrix $\mathbf D_X$  without knowledge of $\mathbf X$ to represent the rows of $\mathbf X$ in a low dimensional space, say 2D or 3D. 
* We search for $\mathbf V_k$ that orthogonally project the rows of $\mathbf X$, $\mathbf x^t_i$  onto a $k-D$ space spanned by the columns of $\mathbf V_k$. In fact we are looking for $\mathbf Z_k$, such that $\mathbf Z_k=\mathbf X \mathbf V_k$
*  But we do not know $\mathbf X$, so how do we get $\mathbf Z_k$? We will use the $\mathbf G_X$ (gram matrix) trick, mentioned in the course notes


## Example: Distances between US cities

As an example, we will use the `UScitiesD` data set, which is part of base R.
This data gives "straight line" distances (in km) between 10 cities in the US.

```{r}
UScitiesD
class(UScitiesD)
```

Note that the `UScitiesD` object is of class `"dist"`, which is a special type of object to represent that it is a __distance matrix__ (we'll denote this as $\mathbf{D}_X$), i.e. the result from computing distances from an original matrix $\mathbf{X}$.
In this case, the original matrix $\mathbf{X}$ was likely a matrix with a row for every city and columns specifying its coordinates.
Note though that we don't know $\mathbf{X}$ exactly.
Still, we can use the distance matrix and MDS to approximate a low-dimensional representation of $\mathbf{X}$.


### Exploring the distance matrix

We first convert the `UScitiesD` to a matrix for easier manipulation and calculation.
Note that this creates a "symmetrical" matrix, with 0s on the diagonal (distance of a city to itself).

```{r}
(dist_mx <- as.matrix(UScitiesD))
```

The dimensions of `dist_mx`:

```{r}
# 10 x 10 square matrix
dim(dist_mx)
```
And the rank of `dist_mx`

```{r} 
qr(dist_mx)$rank
```

>Q: is this matrix of full rank?

<details><summary>Answer</summary>
A: Yes, it is.

```{r}
qr(dist_mx)$rank == min(dim(dist_mx))
```

</details>



### $\mathbf{H}$ and $\mathbf{G}_X$ matrices

Now let's create the $\mathbf  H$ matrix.

$$ \mathbf{H} = \mathbf{I}_{n \times n} - \frac{1}{n} \mathbf{1}_n\mathbf{1}_n^T $$
```{r H_matrix}
n <- nrow(dist_mx)

# 11^T
(one_mat <- matrix(rep(1, n * n), ncol = n, nrow = n))
## Alternatively: one_mat <- rep(1, n) %o% rep(1, n)

## Calculate H, diag(n) is the nxn identity matrix
(H <- diag(n) - (1/n) * one_mat)
```

We can use $\mathbf{H}$ to center our distance matrix:

```{r dist_mx_centered}
(dist_mx_centered <- H %*% dist_mx)
round(colMeans(dist_mx_centered), 8)  # verify colMeans are 0
```

We will use this matrix to calculate $\mathbf{G}_X$ (Gram matrix of $\mathbf{X}$).

$$
\mathbf{G}_X = -\frac{1}{2}\mathbf{H}\mathbf{D}_X\mathbf{H} = \mathbf{X}\mathbf{X}^T
$$

Where $\mathbf{D}_X$ is the matrix of __*squared* distances__.
So we will first have to square our `dist_mx`.

```{r Gram_matrix}
## D_X = squared distance matrix
D_X <- dist_mx ^ 2

## Gram matrix
(G_X <- -1/2 * H %*% (D_X) %*% H)
```


### The SVD

We can now compute the SVD of the Gram matrix and use it to project our original matrix $\mathbf{X}$ (which is still unknown to us!) into a lower dimensional space while preserving the Euclidean distances as well as possible.
This is the essence of MDS.

```{r GX matrix}
## singular value decomposition on gram matrix
Gx_svd <- svd(G_X)

## Use `str` to explore structure of the SVD object
str(Gx_svd)
```

Components of the `Gx_svd` object:

- `Gx_svd$d`: diagonal elements of the $\mathbf{\Delta}$ matrix, to recreate the matrix, use the `diag()` function
- `Gx_svd$u`: the matrix $\mathbf{U}$ of left singular vectors
- `Gx_svd$v`: the matrix $\mathbf{V}$ of right singular vectors


### Truncated SVD and projection into lower dimensional space

The truncated SVD from the Gram matrix can be used to find projections $Z_k$ of $\mathbf{X}$ in a lower dimensional space.
Here we will use $k = 2$.

```{r}
# k=2 approximation
k <- 2
Uk <- Gx_svd$u[, 1:k]
delta_k <- diag(Gx_svd$d[1:k])
Zk <- Uk %*% sqrt(delta_k)
rownames(Zk) <- colnames(D_X)
colnames(Zk) <- c("Z1", "Z2")
Zk

# Plotting Zk in 2-D

## Using base R
# plot(Zk, type = "n", xlab = "Z1", ylab = "Z2", xlim = c(-1500, 1500))
# text(Zk, rownames(Zk), cex = 1.25)

## Using ggplot, by first converting Zk to a tibble
Zk %>%
  # create tibble and change rownames to column named "city"
  as_tibble(rownames = "city") %>% 
  ggplot(aes(Z1, Z2, label = city)) +
    geom_point() +
    # adding the city names as label 
    geom_text(nudge_y = 50) +
    # setting limits of the x-axis to center the plot around 0
    xlim(c(-1500, 1500)) +
    ggtitle("MDS plot of the UScitiesD data") +
    theme_minimal()
```

What can you say about the plot? 
Think about the real locations of these cities on a map of the US.

<details><summary>Answer</summary>
$Z_1$ can be interpreted as the *longitude*, i.e. the East-West position.
$Z_2$ reflects the *latitude*, or the North-South position.
</details>


## The short way

The calculations above demonstrate how MDS works and what the underlying components are.
However, in a real data analysis, one would typically not go through all the hassle of calculating all the intermediate steps.
Fortunately, the MDS is already implemented in base R (in the `stats` package).

So the whole derivation we did above can be reproduced with a single line of code, using the `cmdscale` function (see `?cmdscale` for details).

```{r}
## Calculate MDS in 2 dimensions from distance matrix
(us_mds <- cmdscale(UScitiesD, k = 2))
colnames(us_mds) <- c("Z1", "Z2")

## Plot MDS
us_mds %>% 
  as_tibble(rownames = "city") %>% 
  ggplot(aes(Z1, Z2, label = city)) +
  geom_point() +
  geom_text(nudge_y = 50) +
  xlim(c(-1500, 1500)) +
  theme_minimal()
```

Which gives us the same result as before (which is a good check that we didn't make mistakes!).



# Exercises

## Cheese data

### Data prep {-}

Read in the `cheese` data, which characterizes 30 cheeses on various metrics.

```{r}
cheese <- read_csv("https://github.com/statOmics/HDA2020/raw/data/cheese.csv",
                   col_types = "idddd")
head(cheese)
```

>__Q__: what is the *dimensionality* of this data?

<details><summary>Answer</summary>
__A__: 4, since we have 4 features (the first column is just an identifier for the observation, so we don't regard this as a feature).

```{r}
ncol(cheese) - 1
```

</details>

Convert the `cheese` table to a matrix for easier calculations.
We will drop the first column (as it's not a feature) and instead use it to create `rownames`.

```{r}
cheese_mx <- as.matrix(cheese[, -1])
rownames(cheese_mx) <- paste("case", cheese$Case, sep = "_")
cheese_mx
```

Check rank of `cheese_mx` and center the matrix.

```{r}
qr(cheese_mx)$rank

# Centering the data matrix
n <- nrow(cheese_mx)
cheese_H <- diag(n) - 1 / n * matrix(1, ncol = n, nrow = n)
cheese_centered <- cheese_H %*% as.matrix(cheese_mx)
```


>TO DO: add exercise on linear regression with SVD, using `taste` as the response
> Compare with `lm(taste ~ Acetic + H2S + Lactic, data = cheese)`


### Tasks {-}

*Note*: no need for mathematical derivations, just verify code-wise in R.

 We obtained the column-centered data matrix $\mathbf{X}$ after multiplying the original matrix with

$$
\mathbf{H} = \mathbf{I} - \frac{1}{n} \mathbf{1}\mathbf{1}^T
$$

##### 1. Show that $\mathbf{X}$ (here: `cheese_centered`) is indeed column-centered (and not row-centered) {-}

<details><summary>Solution</summary>
```{r}
# X is indeed column-centered:
round(colMeans(cheese_centered), 14) ## practically zero

# but it is not row-centered:
 rowMeans(cheese_centered)
```
</details>


##### 2. Verify that whenever $\mathbf{X}$ is column-centered, the equality $\mathbf{HX = X}$ holds {-}

<details><summary>Solution</summary>
```{r}
# verifying that HX = X
all.equal(cheese_H %*% cheese_centered, cheese_centered)
```
</details>


##### 3. Perform an SVD on `cheese_centered`, and store the matrices $\mathbf{U}$, $\mathbf{V}$ and $\mathbf{\Delta}$ as separate objects {-}

<details><summary>Solution</summary>
```{r cheese_svd}
cheese_svd <- svd(cheese_centered)
str(cheese_svd)
U <- cheese_svd$u
V <- cheese_svd$v
D <- diag(cheese_svd$d)
```
</details>
  

##### 4. Show that $\mathbf{u_1}$ is a normalized vector; show the same for $\mathbf{u_2}$. Show that $\mathbf{u_1}$ and $\mathbf{u_2}$ are orthogonal vectors. Then show the orthonormality of all vectors $\mathbf{u_j}$ in a single calculation (using the matrix $\mathbf{U}$). Similarly, show the orthonormality of all vectors $\mathbf{v_j}$ in a single calculation (using the matrix $\mathbf{V}$). {-}

<details><summary>Solution</summary>
```{r}
# Verifying orthonormality
# ------------------------
# The vectors u1 and u2 are orthonormal
t(U[, 1]) %*% U[, 1]
t(U[, 2]) %*% U[, 2]
t(U[, 1]) %*% U[, 2]

# Verifying that U forms an orthonormal basis in one step:
t(U) %*% U # computational imperfections
round(t(U) %*% U, digits = 15)


# Verifying that V forms an orthonormal basis:
t(V) %*% V # computational imperfections
round(t(V) %*% V, digits = 15)
```
</details>


##### 5. Check that the SVD was performed correctly, i.e. calculate the matrix $\mathbf{X}$ from the elements of the SVD. {-}

<details><summary>Solution</summary>

There are 2 ways to do this

  - Using the sum definition of the SVD 
    $\mathbf{X} = \sum_{j=1}^r \delta_j \mathbf{u}_j\mathbf{v}_j^T$
  
```{r}
# Calculating X via the sum definition of the SVD:
# ------------------------------------------------

## Initialize empty matrix
X_sum <- matrix(0, nrow = nrow(U), ncol = ncol(V))

## Compute sum by looping over columns
for (j in 1:ncol(U)) {
  X_sum <- X_sum + (diag(D)[j] * U[, j] %*% t(V[, j]))
}
```

  - using the matrix notation of the SVD
  
  $\mathbf{X}=\mathbf{U}_{n\times n}\boldsymbol{\Delta}_{n\times p}\mathbf{V}^T_{p \times p}$
  
```{r}
# Calculating X via the SVD matrix multiplication:
# ------------------------------------------------
X_mult <- U %*% D %*% t(V)
```

  - Verify that the obtained results are identical to the  matrix $\mathbf{X}$.
  
```{r}
## Remove dimnames with unname for comparison
all.equal(X_sum, unname(cheese_centered))
all.equal(X_mult, unname(cheese_centered))
```
</details>

##### 6. Approximate the matrix $\mathbf{\tilde{\mathbf{X}}}$, for $k = 2$ using the truncated SVD. {-}

<details><summary>Solution</summary>

Using the matrix notation of the SVD $\tilde{\mathbf{X}}=\mathbf{U}_{n\times k}\boldsymbol{\Delta}_{k\times k}\mathbf{V}_{p \times k}^T$

```{r}
k <- 2
X_tilde <- U[, 1:k] %*% D[1:k,1:k] %*% t(V[,1:k])
X_tilde
```

- Compare the obtained results with the matrix $\mathbf{X}$ (`cheese_centered`). 
Just at a first glance, does it seem that $\mathbf{\tilde{X}}$ is a good approximation of $\mathbf{X}$?



###Exercise: employment by industry

###In this exercise we will focus on the interpretation of the biplot.

The file ```Industries.txt``` contains data on the distribution of employment between 9 industrial sectors, in 26 European countries. The dataset stems from the Cold-War era; the data are expressed as percentages. Read in the data and have a feel about it.
```{r,echo=FALSE,results=FALSE}
Indus <- read.table("Industries.txt", sep=" ", header=TRUE)
#First 6 rows of data.frame are displayed
head(Indus)

#assign row names
rownames(Indus) <- Indus$country

#delete the first column we will not need it.
X.orig <- Indus[ , -1]

#visualise
head(X.orig)

#Check the dimension
dim(X.orig)

#and then the rank
rankMatrix(X.orig)[[1]] 

#n will be used subsequently
n <- nrow(X.orig)
```


1. Perform the truncated SVD for $k=2$, and construct the biplot accordingly.
```{r,echo=FALSE,results=FALSE}
# Centering the data matrix first
H <- diag(n) -1/n*matrix(1, ncol=n, nrow=n)
X <- H %*% as.matrix(X.orig)
```

```{r,echo=FALSE,results=FALSE}
# Singular Value Decomposition is performed
X.svd <- svd(X)
X.svd
k <- 3
Uk <- X.svd$u[ , 1:k]
Dk <- diag(X.svd$d[1:k])
Zk <- Uk %*% Dk
rownames(Zk) <- Indus$country
Zk
```

```{r,echo=FALSE,results=FALSE}
#Prepare Vk
Vk <- X.svd$v[ , 1:k]
rownames(Vk) <- colnames(Indus[,-1])
Vk
```

```{r,echo=FALSE,results=FALSE}
# # Constructing the biplot for Z1 and Z2
#  # -------------------------------------
# plot(Zk[,1:2], type="n", xlim=c(-30,60), ylim=c(-15,15), 
#       xlab="Z1", ylab="Z2") 
# text(Zk[,1:2], rownames(Zk), cex=0.9)
# # alpha <- 1
# alpha <- 20  # rescaling to get better visualisation
# for(i in 1:9)
# { arrows(0,0, alpha*Vk[i,1], alpha*Vk[i,2], length=0.2, col=2)
#   text(alpha*Vk[i,1], alpha*Vk[i,2], rownames(Vk)[i], col=2)
# }
```

```{r,echo=FALSE,results=FALSE}
#  # Constructing the biplot for Z1 and Z3
# # -------------------------------------
# plot(Zk[,c(1,3)], type="n", xlim=c(-30,60), ylim=c(-15,15), 
#      xlab="Z1", ylab="Z3") 
# text(Zk[,c(1,3)], rownames(Zk), cex=0.9)
# # alpha <- 1
# alpha <- 20  # rescaling to get better visualisation
# for(i in 1:9)
# { arrows(0,0, alpha*Vk[i,1], alpha*Vk[i,3], length=0.2, col=2)
#    text(alpha*Vk[i,1], alpha*Vk[i,3], rownames(Vk)[i], col=2)
# }
 
```

```{r,echo=FALSE,results='hide'}
#  # Constructing the biplot for Z2 and Z3
#  # -------------------------------------
# plot(Zk[,2:3], type="n", xlim=c(-30,60), ylim=c(-15,15), 
#       xlab="Z2", ylab="Z3") 
# text(Zk[,2:3], rownames(Zk), cex=0.9)
# # alpha <- 1
# alpha <- 20  # rescaling to get better visualisation
# for(i in 1:9)
# { arrows(0,0, alpha*Vk[i,2], alpha*Vk[i,3], length=0.2, col=2)
#    text(alpha*Vk[i,2], alpha*Vk[i,3], rownames(Vk)[i], col=2)
# }
#  #We can also use inbuilt R functions for this.
# #--------------------------------------------
# #biplot()
```


2. To see if we can learn more when retaining more dimensions, repeat the truncated SVD for $k=3$. Construct two-dimensional biplots for:
+ Z1 and Z2
+ Z1 and Z3
+ Z2 and Z3

3. Can you give a meaningful interpretation to each dimension? 
Interpretation is crucial, quickly write this on a piece of paper and submit.

# Session info {-}

<details><summary>Session info</summary>

```{r session_info, echo=FALSE, cache=FALSE}
Sys.time()
sessioninfo::session_info()
```

</details>
